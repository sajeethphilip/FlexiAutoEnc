# Autoencoder Configuration File
# ---------------------------------
# This file defines the architecture of the autoencoder.
# Modify the values below to customize the model.

# Input Size
# Format: height width channels
# Example: For RGB images of size 128x128, use "128 128 3"
input_size: 128 128 3

# Encoder Layers
# ---------------------------------
# The encoder compresses the input image into a latent representation.
# Add more layers to increase complexity.
# Supported layer types: conv, maxpool

# Convolutional Layer
# Format: conv: out_channels=<int>, kernel_size=<int>, stride=<int>, padding=<int>, activation=<relu|sigmoid|tanh>
# Example: conv: out_channels=32, kernel_size=3, stride=1, padding=1, activation=relu
encoder:
  conv: out_channels=32, kernel_size=3, stride=1, padding=1, activation=relu

# Max Pooling Layer
# Format: maxpool: kernel_size=<int>, stride=<int>
# Example: maxpool: kernel_size=2, stride=2
  maxpool: kernel_size=2, stride=2

# Add more convolutional and pooling layers to increase depth and complexity.
  conv: out_channels=64, kernel_size=3, stride=1, padding=1, activation=relu
  maxpool: kernel_size=2, stride=2

# Decoder Layers
# ---------------------------------
# The decoder reconstructs the image from the latent representation.
# Add more layers to increase complexity.
# Supported layer types: convtranspose

# Transposed Convolutional Layer
# Format: convtranspose: out_channels=<int>, kernel_size=<int>, stride=<int>, activation=<relu|sigmoid|tanh>
# Example: convtranspose: out_channels=32, kernel_size=2, stride=2, activation=relu
decoder:
  convtranspose: out_channels=32, kernel_size=2, stride=2, activation=relu
  convtranspose: out_channels=3, kernel_size=2, stride=2, activation=sigmoid

# Add more transposed convolutional layers to increase depth and complexity.

# Advanced Options (Uncomment to use)
# ---------------------------------
# To add batch normalization or dropout, uncomment and modify the lines below.
# Note: You will need to extend the parser and model builder to support these features.

# Batch Normalization Layer
# Format: batchnorm: num_features=<int>
# Example: batchnorm: num_features=32
#  batchnorm: num_features=32

# Dropout Layer
# Format: dropout: p=<float>
# Example: dropout: p=0.5
#  dropout: p=0.5
